# ğŸš€ nnx - Neural Network Executor

<div align="center">

**A blazing-fast GPU-accelerated ONNX inference engine written in Rust** ğŸ¦€âš¡

[![Rust](https://img.shields.io/badge/Rust-2024-orange.svg)](https://www.rust-lang.org/)
[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-0.0.0-green.svg)](https://github.com/yourusername/nnx)

</div>

---

## âœ¨ Features

- ğŸ® **GPU Acceleration** - Powered by Burn and wgpu for high-performance inference
- ğŸ“¦ **ONNX Support** - Run models trained in PyTorch, TensorFlow, and more
- ğŸ”¥ **Rust Performance** - Memory-safe and lightning fast
- ğŸ–¼ï¸ **Image Processing** - Built-in image loading and preprocessing
- ğŸ·ï¸ **Easy Inference** - Simple CLI for model inference
- ğŸ”Œ **Multi-device** - Support for multiple GPU devices

---

## ğŸ› ï¸ Installation

### Prerequisites

- ğŸ¦€ Rust 2024 edition or later
- ğŸ”§ Cargo (comes with Rust)

### Build from source

```bash
git clone https://github.com/yourusername/nnx.git
cd nnx
cargo build --release
```

---

## ğŸš€ Getting Started

### List Available Devices

Check what GPUs are available on your system:

```bash
cargo run --release -- devices
```

Output example:
```
ğŸ® Available GPU Devices:
  [0] NVIDIA GeForce RTX 3080
  [1] AMD Radeon RX 6800 XT
```

### Model Information

Get detailed info about an ONNX model:

```bash
cargo run --release -- info model.onnx
```

### Run Inference

Perform inference on an image with a model:

```bash
cargo run --release -- infer model.onnx \
  --input sample.jpg \
  --labels imagenet_labels.txt \
  --top 5
```

Output example:
```
ğŸ”® Running inference...
âœ¨ Top 5 predictions:
  1. [0.9532] Golden Retriever ğŸ•
  2. [0.0231] Labrador Retriever ğŸ•
  3. [0.0124] Cocker Spaniel ğŸ•
  4. [0.0087] English Setter ğŸ•
  5. [0.0023] Brittany ğŸ•
```

---

## ğŸ—ï¸ Architecture

nnx is built with cutting-edge Rust libraries:

- ğŸ“¥ **ONNX IR** - Parse and understand ONNX models
- âš¡ **Burn** - High-performance tensor operations
- ğŸ® **wgpu** - Cross-platform GPU computing
- ğŸ–¼ï¸ **Image** - Image loading and processing
- ğŸ¯ **Clap** - Beautiful CLI interface
- ğŸ›¡ï¸ **thiserror** - Error handling

---

## ğŸ§ª Supported Operations

âœ… Arithmetic operations (Add, Sub, Mul, Div, etc.)
âœ… Activation functions (ReLU, GELU, Tanh, Sigmoid, etc.)
âœ… Convolution layers (1D, 2D)
âœ… Normalization layers (BatchNorm, LayerNorm)
âœ… Pooling operations (MaxPool, AvgPool)
âœ… Reduction operations (ReduceSum, ReduceMean, etc.)
âœ… Shape operations (Reshape, Transpose, etc.)
âœ… Matrix operations (MatMul, Gemm)
âœ… Unary operations (Abs, Neg, Sqrt, Exp, Log, etc.)
âœ… Comparison operations (Equal, Greater, Less, etc.)
âœ… RNN/LSTM/GRU layers
âœ… Audio operations
âœ… And many more! ğŸ‰

---

## âš ï¸ Known Limitations

### ğŸ”¸ 3D Convolution Support (Framework Limitation)

**Conv3d and ConvTranspose3d are not currently supported** â›”

Due to limitations in the underlying Burn `DynTensor` framework (which only supports tensors up to rank-4), 3D convolution operations requiring rank-5 tensors cannot be executed. Models that use 3D convolutions (typically for video processing or medical imaging) will not run until either:
- The Burn framework is extended to support rank-5+ tensors, or
- An alternative tensor abstraction is implemented

**What works:**
- âœ… Conv1d (audio/time-series)
- âœ… Conv2d (image classification, detection)
- âœ… ConvTranspose1d/2d

**What doesn't work:**
- â›” Conv3d (video, volumetric data)
- â›” ConvTranspose3d

---

### ğŸ”¸ Large Model Parsing (Parser Issues)

**Some larger models may fail to parse due to bugs in the ONNX parser** ğŸ›

The `onnx-ir` library (used for parsing ONNX models) has known issues that can prevent loading certain models, particularly larger or more complex architectures.

**Workarounds:**
- Try simpler models with fewer operators
- Consider re-exporting models with minimal operator sets
- Use ONNX simplification tools to reduce model complexity

---

## ğŸ“– Example Workflow

Here's a complete example using ResNet-18:

1. **Check devices** ğŸ‘ˆ
   ```bash
   cargo run --release -- devices
   ```

2. **Inspect model** ğŸ“‹
   ```bash
   cargo run --release -- info test_data/resnet18.onnx
   ```

3. **Run inference** ğŸ”®
   ```bash
   cargo run --release -- infer test_data/resnet18.onnx \
     --input test_data/sample.jpg \
     --labels test_data/imagenet_labels.txt \
     --top 5
   ```

---

## ğŸ§‘â€ğŸ’» Development

### Run tests

```bash
cargo test
```

### Build documentation

```bash
cargo doc --open
```

### Format code

```bash
cargo fmt
```

---

## ğŸ¤ Contributing

We love contributions! ğŸ‰

1. Fork the repository ğŸ´
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request ğŸš€

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- ğŸ’ª The [Burn](https://burn.dev/) team for the amazing deep learning framework
- ğŸ® The [wgpu](https://github.com/gfx-rs/wgpu) team for the incredible GPU abstraction
- ğŸ“¦ The [ONNX](https://onnx.ai/) community for the open model format
- ğŸ¦€ The Rust community for the awesome language and ecosystem

---

## ğŸ“§ Contact

For questions, suggestions, or just to say hi ğŸ‘‹:
- Open an issue on GitHub
- Reach out via Discussions

---

<div align="center">

Made with â¤ï¸ and ğŸ¦€

**[â¬† Back to Top](#-nnx---neural-network-executor)**

</div>